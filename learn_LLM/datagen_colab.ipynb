{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce76a09",
   "metadata": {},
   "source": [
    "Colab ì „ìš© ë°ì´í„°ì…‹ ìƒì„± ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122e20d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install -q -U langchain-core langchain-community langchain-text-splitters pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdee94f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.colab import userdata, drive\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PDF ì²˜ë¦¬ìš©\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# API í‚¤ ì„¤ì •\n",
    "api_key = userdata.get('vla')\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9bd67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# PDF ê²½ë¡œ ì§€ì • (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
    "pdf_filename = \"/content/drive/MyDrive/js_mujoco/learn_LLM/src/6g_ai.pdf\"\n",
    "\n",
    "if os.path.exists(pdf_filename):\n",
    "    print(f\"íŒŒì¼ ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ: {pdf_filename}\")\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë¡œë“œ\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_filename)\n",
    "        docs = loader.load()\n",
    "        print(f\"ë¬¸ì„œ ë¡œë“œ ì„±ê³µ! ì´ {len(docs)} í˜ì´ì§€\")\n",
    "    except Exception as e:\n",
    "        print(f\"ë¬¸ì„œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "else:\n",
    "    print(f\" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78fa18a",
   "metadata": {},
   "source": [
    "***í•¨ìˆ˜ ì •ì˜***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14999d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. PDF ë¡œë“œ ë° ë¶„í• \n",
    "def load_and_split_pdf(file_path):\n",
    "    print(f\"Loading PDF with PyMuPDF: {file_path}...\")\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d671e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. ë°ì´í„°ì…‹ ìƒì„± ë£¨í”„\n",
    "def generate_dataset_gemini(chunks, output_file=\"dataset.jsonl\"):\n",
    "    final_dataset = []\n",
    "\n",
    "    base_prompt = \"\"\"\n",
    "    ë‹¹ì‹ ì€ 6G ì´ë™í†µì‹ ê³¼ AI ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "    ì•„ë˜ [Context]ë¥¼ ë³´ê³  í•™ìŠµìš© ì§ˆë¬¸-ë‹µë³€ ìŒ 3ê°œë¥¼ JSONìœ¼ë¡œ ë§Œë“œì„¸ìš”.\n",
    "\n",
    "    [Context] (Page {page_num})\n",
    "    {context}\n",
    "\n",
    "    [Format]\n",
    "    {{ \"dataset\": [ {{ \"instruction\": \"...\", \"output\": \"...\" }} ] }}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\" ë°ì´í„° ìƒì„± ì‹œì‘! (ì´ {len(chunks)} ì²­í¬ ì²˜ë¦¬ ì˜ˆì •)\")\n",
    "\n",
    "    for chunk in tqdm(chunks):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model='gemini-2.5-flash',\n",
    "                contents=base_prompt.format(\n",
    "                    context=chunk.page_content,\n",
    "                    page_num=chunk.metadata.get('page', 0) + 1\n",
    "                ),\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0.5,\n",
    "                    response_mime_type=\"application/json\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            data = json.loads(response.text)\n",
    "            if \"dataset\" in data:\n",
    "                for item in data[\"dataset\"]:\n",
    "                    final_dataset.append({\n",
    "                        \"instruction\": item[\"instruction\"],\n",
    "                        \"input\": \"\",\n",
    "                        \"output\": item[\"output\"]\n",
    "                    })\n",
    "            time.sleep(3) # Rate Limit ë°©ì§€\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            continue\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in final_dataset:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n ì €ì¥ ì™„ë£Œ: {output_file} (ì´ {len(final_dataset)}ê°œ ë°ì´í„° ìŒ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257b93e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. ë°ì´í„° ì¦ê°• ë£¨í”„\n",
    "def augment_dataset_gemini(input_file, output_file):\n",
    "    # ì…ë ¥ íŒŒì¼ ì½ê¸°\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        original_data = [json.loads(line) for line in f]\n",
    "\n",
    "    augmented_data = []\n",
    "\n",
    "    aug_prompt = \"\"\"\n",
    "    ì•„ë˜ ì£¼ì–´ì§„ ì§ˆë¬¸(Instruction)ê³¼ ë‹µë³€(Output)ì„ ë³´ê³ ,\n",
    "    **ë‹µë³€ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ ì§ˆë¬¸ì˜ í‘œí˜„ë§Œ ë‹¤ë¥´ê²Œ ë°”ê¾¼ ìœ ì‚¬ ì§ˆë¬¸ 3ê°œ**ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "    [Original]\n",
    "    Q: {instruction}\n",
    "    A: {output}\n",
    "\n",
    "    [Format]\n",
    "    {{ \"variations\": [\"ìœ ì‚¬ ì§ˆë¬¸ 1\", \"ìœ ì‚¬ ì§ˆë¬¸ 2\", \"ìœ ì‚¬ ì§ˆë¬¸ 3\"] }}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\" ë°ì´í„° ì¦ê°• ì‹œì‘! (ì›ë³¸ {len(original_data)}ê°œ -> ëª©í‘œ ì•½ {len(original_data)*4}ê°œ)\")\n",
    "\n",
    "    # ì§„í–‰ë°” í‘œì‹œ\n",
    "    for i, entry in enumerate(tqdm(original_data)):\n",
    "        augmented_data.append(entry) # ì›ë³¸ ë°ì´í„°ë„ í¬í•¨\n",
    "\n",
    "        try:\n",
    "            # Gemini í˜¸ì¶œ\n",
    "            response = client.models.generate_content(\n",
    "                model='gemini-2.5-flash', \n",
    "                contents=aug_prompt.format(\n",
    "                    instruction=entry['instruction'],\n",
    "                    output=entry['output']\n",
    "                ),\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0.7,\n",
    "                    response_mime_type=\"application/json\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # ì‘ë‹µ íŒŒì‹±\n",
    "            data = json.loads(response.text)\n",
    "\n",
    "            if \"variations\" in data:\n",
    "                count = 0\n",
    "                for new_q in data[\"variations\"]:\n",
    "                    augmented_data.append({\n",
    "                        \"instruction\": new_q,\n",
    "                        \"input\": \"\",\n",
    "                        \"output\": entry['output']\n",
    "                    })\n",
    "                    count += 1\n",
    "                # (ì˜µì…˜) ì˜ ë˜ê³  ìˆëŠ”ì§€ í™•ì¸ìš© ë¡œê·¸ (ë„ˆë¬´ ë§ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬)\n",
    "                # print(f\"[{i}] ì¦ê°• +{count}ê°œ ì™„ë£Œ\")\n",
    "            else:\n",
    "                print(f\" [{i}] ì‘ë‹µì— 'variations' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {data}\")\n",
    "\n",
    "            time.sleep(3) # API ì œí•œ ë°©ì§€\n",
    "\n",
    "        except Exception as e:\n",
    "            # [ì¤‘ìš”] ì—ëŸ¬ê°€ ë‚˜ë©´ ì´ìœ ë¥¼ ì¶œë ¥í•´ì•¼ í•¨\n",
    "            print(f\" [{i}] ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "            continue\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in augmented_data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n ì¦ê°• ì™„ë£Œ! ì´ {len(augmented_data)}ê°œ ë°ì´í„°ê°€ '{output_file}'ì— ì €ì¥ë¨.\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "augment_dataset_gemini(\"6g_ai_dataset.jsonl\", \"6g_ai_dataset_augmented.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dad30",
   "metadata": {},
   "source": [
    "ë©”ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce2f82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. ë¡œë“œ ë° ë¶„í• \n",
    "chunks = load_and_split_pdf(pdf_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62575d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. ì´ˆê¸° ë°ì´í„°ì…‹ ìƒì„±\n",
    "generate_dataset_gemini(chunks, \"6g_ai_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59edc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. ë°ì´í„° ì¦ê°•\n",
    "augment_dataset_gemini(\"6g_ai_dataset.jsonl\", \"6g_ai_dataset_augmented.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f45a8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# 4. ê²°ê³¼ íŒŒì¼ Google Driveë¡œ ì €ì¥\n",
    "\n",
    "# PDF íŒŒì¼ì´ ìˆë˜ í´ë” ê²½ë¡œë¥¼ êµ¬í•©ë‹ˆë‹¤ (ì˜ˆ: /content/drive/MyDrive/...)\n",
    "save_dir = os.path.dirname(pdf_filename)\n",
    "\n",
    "# ì €ì¥í•  ì „ì²´ ê²½ë¡œ ì„¤ì •\n",
    "target_file_1 = os.path.join(save_dir, \"6g_ai_dataset.jsonl\")\n",
    "target_file_2 = os.path.join(save_dir, \"6g_ai_dataset_augmented.jsonl\")\n",
    "\n",
    "print(f\"ğŸ“‚ ê²°ê³¼ íŒŒì¼ì„ Google Drive í´ë”ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\\nê²½ë¡œ: {save_dir}\")\n",
    "\n",
    "# íŒŒì¼ ì´ë™ (ë˜ëŠ” ë³µì‚¬)\n",
    "if os.path.exists(\"6g_ai_dataset.jsonl\"):\n",
    "    shutil.copy(\"6g_ai_dataset.jsonl\", target_file_1)\n",
    "    print(f\" ì €ì¥ ì™„ë£Œ: {target_file_1}\")\n",
    "else:\n",
    "    print(\" ì›ë³¸ ë°ì´í„°ì…‹ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if os.path.exists(\"6g_ai_dataset_augmented.jsonl\"):\n",
    "    shutil.copy(\"6g_ai_dataset_augmented.jsonl\", target_file_2)\n",
    "    print(f\" ì €ì¥ ì™„ë£Œ: {target_file_2}\")\n",
    "else:\n",
    "    print(\" ì¦ê°• ë°ì´í„°ì…‹ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
